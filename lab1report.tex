\documentclass[11pt]{article}
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{times}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\hypersetup{colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue}

\title{BubbleReduce: Communication-Aware and Online Pipeline Partitioning to Reduce Pipeline Bubbles}
\author{Anonymous}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
Pipeline parallel training suffers from hidden bubbles when stage runtimes are heterogeneous and backward passes are slower than forward passes. Common systems assume uniform stages and symmetric forward/backward (F/B) costs, approximating the bubble fraction by $(p-1)/m$ for $p$ stages and $m$ microbatches, which often underestimates tail stalls. We present two practical techniques: (1) a profile-driven, communication-aware partitioner that balances the sum of forward and backward times across stages while penalizing large boundary activations; and (2) an online, micro-adjustment mechanism that continuously monitors stage runtimes and carefully moves one or two small blocks across boundaries with cooldown, resource guards, and rollback. Using a toy MLP with heterogeneous layers, we show consistent reduces in makespan and bubble fraction under both GPipe and 1F1B schedules, and demonstrate how compensating F/B asymmetry further improves balance in the backward phase.
\end{abstract}

\section{Introduction}
\label{sec:intro}
Pipeline parallelism partitions a model into $p$ stages and splits a batch into $m$ microbatches to overlap execution. Under ideal assumptions (equal forward/backward times and uniform stage costs), the bubble fraction is often approximated by $(p-1)/m$. Real networks violate these assumptions: different layers have different runtimes and memory footprints; backward is usually slower than forward; and communication across boundaries can be significant. These effects produce long-tail stalls and hidden bubbles that dominate end-to-end throughput.

This work takes a pragmatic approach.
First, we measure per-layer forward ($f_i$) and backward ($b_i$) times and group layers into blocks to avoid overly fine partitions.
We then partition blocks across $p$ stages to balance compute and reduce communication, using either a greedy heuristic or a dynamic-programming (DP) optimum that minimizes the maximum stage time and, among feasible splits, minimizes boundary communication.
Second, we add an online micro-adjustment mechanism that reacts to runtime drift by moving one or two small blocks across adjacent boundaries under strict safeguards (cooldown, resource guards, and rollback).
We also test an F/B imbalance compensation variant that uses $\sum_i (f_i + \alpha\, b_i)$ with $\alpha>1$ so backward-heavy blocks do not cluster in one stage.

All code is included in this repository:
\texttt{pipeline\_partition\_demo.py} (partitioning + scheduling simulator), \texttt{online\_adjust.py} (online micro-adjustment), and \texttt{balance.py} (F/B compensation).

\section{Method}
\label{sec:method}

\subsection{Method A: Profile-Driven, Communication-Aware Partitioning}
\label{sec:methodA}
We profile a sequential model layer-by-layer to obtain: (i) average forward time $f_i$, (ii) average backward time $b_i$, and (iii) activation size at layer outputs (used as a boundary communication proxy).
Layers are grouped into blocks (e.g., Linear+ReLU) to control granularity.

Given per-block compute $c_j = \sum_{i\in j}(f_i + b_i)$ and boundary proxy $s_j$ (output activation of the last layer in a block), we partition blocks into $p$ stages.
We provide two strategies:
\begin{itemize}
  \item Greedy compute-balancing with communication penalty: place cuts near the equal-load target while penalizing large $s_j$ boundaries.
  \item DP optimal: minimize the maximum stage time (makespan proxy) and, among all feasible partitions, minimize the sum of boundary costs. A slack variant allows a small imbalance to further reduce communication.
\end{itemize}
The chosen partition is then evaluated under GPipe and 1F1B schedulers using a simple list-scheduling simulator that models F/B durations per stage and communication at boundaries for each microbatch.

\paragraph{F/B Imbalance Compensation.}
We also evaluate a variant using $c_j^{(\alpha)} = \sum_{i\in j}(f_i + \alpha b_i)$ with $\alpha>1$ to explicitly counter backward slowness and reduce backward-phase tail stalls.

\subsection{Method B: Online Boundary Micro-Adjustment}
\label{sec:methodB}
Starting from a static partition, we simulate training over multiple steps with per-block jitter and small drift.
We monitor rolling per-stage busy time to identify laggards.
When not throttled by resource guards (VRAM proxy and PCIe ratio), the system proposes moving one or two small blocks across the boundary with the laggard stage.
Each candidate move is predicted using the same simulator; a move is committed only if it improves the predicted step time by a small threshold.
A cooldown prevents oscillation; if the rolling average fails to improve after a patience window, the move is rolled back.

\section{3. Results and Discussion}
\label{sec:results}
We evaluate a toy MLP (heterogeneous hidden sizes) on CPU to highlight heterogeneity and F/B asymmetry.
Figures and CSV are generated by the scripts (see \texttt{figs/}, \texttt{figs\_online/}, and \texttt{figs\_fb/}).

\subsection{Method A Results: Static, Communication-Aware Partitioning}
\label{sec:resA}
\begin{figure}[t]
  \centering
  \includegraphics[width=0.48\textwidth]{figs/makespan_gpipe_300.png}\hfill
  \includegraphics[width=0.48\textwidth]{figs/bubble_gpipe_300.png}
  \caption{Method A under GPipe at 300 GB/s: makespan and bubble vs. microbatches. DP (ours) consistently outperforms uniform partitioning.}
  \label{fig:gpipe300}
\end{figure}

Using the DP partitioner, we observe consistent makespan reductions versus uniform blocks while accounting for communication.
Typical improvements on the toy MLP include (examples from generated outputs):
\begin{itemize}
  \item GPipe, $m=8$, 300 GB/s: speedup $\approx\,1.28\times$ (bubble reduces from $\sim$45.7\% to $\sim$30.1\%).
  \item GPipe, $m=16$, 300 GB/s: speedup $\approx\,1.30\times$.
  \item 1F1B, $m=8$, 300 GB/s: speedup $\approx\,1.19\times$ (bubble reduces from $\sim$46.1\% to $\sim$35.6\%).
\end{itemize}
These gains hold at lower bandwidth (e.g., 50 GB/s), where the communication-aware objective helps avoid overly costly boundaries.
Greedy balancing provides similar, slightly weaker gains.

\paragraph{Effect of F/B Compensation.}
Weighting backward more (e.g., $\alpha=2$) further reduces the maximum backward load across stages, improving the backward phase balance; stacked F/B stage plots (\texttt{figs\_fb/stage\_loads\_*.png}) visualize this effect.

\subsection{Method B Results: Online Micro-Adjustment}
\label{sec:resB}
\begin{figure}[t]
  \centering
  \includegraphics[width=0.48\textwidth]{figs_online/makespan_timeseries.png}\hfill
  \includegraphics[width=0.48\textwidth]{figs_online/makespan_rolling.png}
  \caption{Method B timeseries: Online (red) vs. Static (gray). Green triangles mark committed moves; orange X mark rollbacks. Online tracks lower as drift accumulates.}
  \label{fig:online}
\end{figure}

Under realistic runtime drift, the online tuner performs small boundary nudges that keep stages balanced. The improvements are modest when the static split already matches the workload, but grow as drift accumulates. The cooldown, resource guards, and rollback keep the process stable and predictable.
We report per-step makespan and bubble, plus a summary with mean/median/p95, speedups, and move/rollback counts (\texttt{figs\_online/summary.txt}).

\section{4. Conclusion}
We presented two complementary techniques to reduce pipeline bubbles in the presence of heterogeneous runtimes and F/B asymmetry. A profile-driven, communication-aware partitioner with a DP-based objective brings consistent gains in both GPipe and 1F1B schedules. An online micro-adjustment mechanism further adapts to runtime drift with minimal overhead and strong safeguards. Explicitly compensating backward cost concentrates less backward work in a single stage, reducing tail stalls. Together, these techniques improve makespan and utilization beyond idealized assumptions that treat stages and F/B costs as uniform.

\end{document}

